{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 09:16:53.461272: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /app/lib\n",
      "2021-11-30 09:16:53.461394: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above we use the load function from the spacy library to load the core English language model. The model is stored in the sp variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a small document using this model. A document can be a sentence or a group of sentences and can have unlimited length. The following script creates a simple spaCy document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sp(u'Manchester United is looking to sign a forward for $90 million')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy automatically breaks your document into tokens when a document is created using the model.\n",
    "\n",
    "A token simply refers to an individual part of a sentence having some semantic value. Let's see what tokens we have in our document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester\n",
      "United\n",
      "is\n",
      "looking\n",
      "to\n",
      "sign\n",
      "a\n",
      "forward\n",
      "for\n",
      "$\n",
      "90\n",
      "million\n"
     ]
    }
   ],
   "source": [
    "for word in sentence:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see we have the following tokens in our document. We can also see the parts of speech of each of these tokens using the .pos_ attribute shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester PROPN\n",
      "United PROPN\n",
      "is AUX\n",
      "looking VERB\n",
      "to PART\n",
      "sign VERB\n",
      "a DET\n",
      "forward NOUN\n",
      "for ADP\n",
      "$ SYM\n",
      "90 NUM\n",
      "million NUM\n"
     ]
    }
   ],
   "source": [
    "for word in sentence:\n",
    "    print(word.text,  word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that each word or token in our sentence has been assigned a part of speech. For instance \"Manchester\" has been tagged as a proper noun, \"Looking\" has been tagged as a verb, and so on.\n",
    "\n",
    "Finally, in addition to the parts of speech, we can also see the dependencies.\n",
    "\n",
    "Let's create another document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = sp(u\"Manchester United isn't looking to sign any forward.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dependency parsing, the attribute dep_ is used as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester PROPN compound\n",
      "United PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "looking VERB ROOT\n",
      "to PART aux\n",
      "sign VERB xcomp\n",
      "any DET det\n",
      "forward ADV advmod\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "for word in sentence2:\n",
    "    print(word.text,  word.pos_, word.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, you can see that spaCy is intelligent enough to find the dependency between the tokens, for instance in the sentence we had a word is'nt. The depenency parser has broken it down to two words and specifies that the n't is actually negation of the previous word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to printing the words, you can also print sentences from a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = sp(u'Hello from Stackabuse. The site with the best Python Tutorials. What are you looking for?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can iterate through each sentence using the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from Stackabuse.\n",
      "The site with the best Python Tutorials.\n",
      "What are you looking for?\n"
     ]
    }
   ],
   "source": [
    "for sentence in document.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check if a sentence starts with a particular token or not. You can get individual tokens using an index and the square brackets, like an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to see if any sentence in the document starts with The, we can use the is_sent_start attribute as shown below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[4].is_sent_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"They're leaving U.K. for U.S.A.\"\n"
     ]
    }
   ],
   "source": [
    "sentence3 = sp(u'\"They\\'re leaving U.K. for U.S.A.\"')\n",
    "print(sentence3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the sentence contains quotes at the beginnnig and at the end. It also contains punctuation marks in abbreviations \"U.K\" and \"U.S.A.\"\n",
    "\n",
    "Let's see how spaCy tokenizes this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "They\n",
      "'re\n",
      "leaving\n",
      "U.K.\n",
      "for\n",
      "U.S.A.\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "for word in sentence3:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output, you can see that spaCy has tokenized the starting and ending double quotes. However, it is intelligent enough, not to tokenize the punctuation dot used between the abbreviations such as U.K. and U.S.A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am non-vegetarian, email me the menu at abc-xyz@gmai.com\n"
     ]
    }
   ],
   "source": [
    "sentence4 = sp(u\"Hello, I am non-vegetarian, email me the menu at abc-xyz@gmai.com\")\n",
    "print(sentence4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in the above sentence, we have a dash in the word \"non-vegetarian\" and in the email address. Let's see how spaCy will tokenize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "I\n",
      "am\n",
      "non\n",
      "-\n",
      "vegetarian\n",
      ",\n",
      "email\n",
      "me\n",
      "the\n",
      "menu\n",
      "at\n",
      "abc-xyz@gmai.com\n"
     ]
    }
   ],
   "source": [
    "for word in sentence4:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident from the output that spaCy was actually able to detect the email and it did not tokenize it despite having a \"-\". On the other hand, the word \"non-vegetarian\" was tokenized.\n",
    "\n",
    "Let's now see how we can count the words in the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to tokenizing the documents to words, you can also find if the word is an entity such as a company, place, building, currency, institution, etc.\n",
    "\n",
    "Let's see a simple example of named entity recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence5 = sp(u'Manchester United is looking to sign Harry Kane for $90 million') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester\n",
      "United\n",
      "is\n",
      "looking\n",
      "to\n",
      "sign\n",
      "Harry\n",
      "Kane\n",
      "for\n",
      "$\n",
      "90\n",
      "million\n"
     ]
    }
   ],
   "source": [
    "for word in sentence5:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that \"Manchester United\" is a single word, therefore it should not be tokenized into two words. Similarly, \"Harry Kane\" is the name of a person, and \"$90 million\" is a currency value. These should not be tokenized either.\n",
    "\n",
    "This is where named entity recognition comes to play. To get the named entities from a document, you have to use the ents attribute. Let's retrieve the named entities from the above sentence. Execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester United - GPE - Countries, cities, states\n",
      "Harry Kane - PERSON - People, including fictional\n",
      "$90 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for entity in sentence5.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to detecting named entities, nouns can also be detected. To do so, the noun_chunks attribute is used. Consider the following sentence:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence5 = sp(u'Latest Rumours: Manchester United is looking to sign Harry Kane for $90 million') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Rumours\n",
      "Manchester United\n",
      "Harry Kane\n"
     ]
    }
   ],
   "source": [
    "for noun in sentence5.noun_chunks:\n",
    "    print(noun.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming refers to reducing a word to its root form. While performing natural language processing tasks, you will encounter various scenarios where you find different words with the same root. For instance, compute, computer, computing, computed, etc. You may want to reduce the words to their root form for the sake of uniformity. This is where stemming comes in to play.\n",
    "\n",
    "It might be surprising to you but spaCy doesn't contain any function for stemming as it relies on lemmatization only. Therefore, in this section, we will use NLTK for stemming.\n",
    "\n",
    "There are two types of stemmers in NLTK: Porter Stemmer and Snowball stemmers. Both of them have been implemented using different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['compute', 'computer', 'computed', 'computing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute --> comput\n",
      "computer --> comput\n",
      "computed --> comput\n",
      "computing --> comput\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token + ' --> ' + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all the 4 words have been reduced to \"comput\" which actually isn't a word at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowball Stemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowball stemmer is a slightly improved version of the Porter stemmer and is usually preferred over the latter. Let's see snowball stemmer in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute --> comput\n",
      "computer --> comput\n",
      "computed --> comput\n",
      "computing --> comput\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "tokens = ['compute', 'computer', 'computed', 'computing']\n",
    "\n",
    "for token in tokens:\n",
    "    print(token + ' --> ' + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the results are the same. We still got \"comput\" as the stem. Again, this word \"comput\" actually isn't a dictionary word.\n",
    "\n",
    "This is where lemmatization comes handy. Lemmatization reduces the word to its stem as it appears in the dictionary. The stems returned through lemmatization are actual dictionary words and are semantically complete unlike the words returned by stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we could not perform stemming with spaCy, we can perform lemmatization using spaCy.\n",
    "\n",
    "To do so, we need to use the lemma_ attribute on the spaCy document. Suppose we have the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence6 = sp(u'compute computer computed computing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute compute\n",
      "computer computer\n",
      "computed compute\n",
      "computing computing\n"
     ]
    }
   ],
   "source": [
    "for word in sentence6:\n",
    "    print(word.text,  word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that unlike stemming where the root we got was \"comput\", the roots that we got here are actual words in the dictionary.\n",
    "\n",
    "Lemmatization converts words in the second or third forms to their first form variants. Look at the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A  ===> a\n",
      "letter  ===> letter\n",
      "has  ===> have\n",
      "been  ===> be\n",
      "written  ===> write\n",
      ",  ===> ,\n",
      "asking  ===> ask\n",
      "him  ===> he\n",
      "to  ===> to\n",
      "be  ===> be\n",
      "released  ===> release\n"
     ]
    }
   ],
   "source": [
    "sentence7 = sp(u'A letter has been written, asking him to be released')\n",
    "\n",
    "for word in sentence7:\n",
    "    print(word.text + '  ===>', word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Example_Sentence = \"Patients who in late middle age have smoked 20 cigarettes a day since their teens constitute an at-risk group. One thing they’re clearly at risk for is the acute sense of guilt that a clinician can incite, which immediately makes a consultation tense.\"\n",
    "#You might need to download the model when you first load the model, follow the instruction given in the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_process(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "#Tokenization and lemmatization are done with the spacy nlp pipeline commands\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    print(\"Tokenize+Lemmatize:\")\n",
    "    print(lemma_list)\n",
    "    \n",
    "    #Filter the stopword\n",
    "    filtered_sentence =[] \n",
    "    for word in lemma_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word) \n",
    "    \n",
    "    #Remove punctuation\n",
    "    punctuations=\"?:!.,;\"\n",
    "    for word in filtered_sentence:\n",
    "        if word in punctuations:\n",
    "            filtered_sentence.remove(word)\n",
    "    print(\" \")\n",
    "    print(\"Remove stopword & punctuation: \")\n",
    "    print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize+Lemmatize:\n",
      "['patient', 'who', 'in', 'late', 'middle', 'age', 'have', 'smoke', '20', 'cigarette', 'a', 'day', 'since', 'their', 'teen', 'constitute', 'an', 'at', '-', 'risk', 'group', '.', 'one', 'thing', 'they', '’re', 'clearly', 'at', 'risk', 'for', 'be', 'the', 'acute', 'sense', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incite', ',', 'which', 'immediately', 'make', 'a', 'consultation', 'tense', '.']\n",
      " \n",
      "Remove stopword & punctuation: \n",
      "['patient', 'late', 'middle', 'age', 'smoke', '20', 'cigarette', 'day', 'teen', 'constitute', '-', 'risk', 'group', 'thing', 'clearly', 'risk', 'acute', 'sense', 'guilt', 'clinician', 'incite', 'immediately', 'consultation', 'tense']\n"
     ]
    }
   ],
   "source": [
    "spacy_process(Example_Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "#s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "def nltk_process(text):\n",
    "    #Tokenization\n",
    "    nltk_tokenList = word_tokenize(text)\n",
    "    \n",
    "    #Stemming\n",
    "    nltk_stemedList = []\n",
    "    for word in nltk_tokenList:\n",
    "        nltk_stemedList.append(p_stemmer.stem(word))\n",
    "        #nltk_stemedList.append(s_stemmer.stem(word))\n",
    "    \n",
    "    #Lemmatization\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    nltk_lemmaList = []\n",
    "    for word in nltk_stemedList:\n",
    "        nltk_lemmaList.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    \n",
    "    print(\"Stemming + Lemmatization\")\n",
    "    print(nltk_lemmaList)\n",
    "\n",
    "    #Filter stopword\n",
    "    filtered_sentence = []  \n",
    "    nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "    for w in nltk_lemmaList:  \n",
    "        if w not in nltk_stop_words:  \n",
    "            filtered_sentence.append(w)  \n",
    "\n",
    "    #Removing Punctuation\n",
    "    punctuations=\"?:!.,;\"\n",
    "    for word in filtered_sentence:\n",
    "        if word in punctuations:\n",
    "            filtered_sentence.remove(word)\n",
    "    print(\" \")\n",
    "    print(\"Remove stopword & Punctuation\")\n",
    "    print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming + Lemmatization\n",
      "['patient', 'who', 'in', 'late', 'middl', 'age', 'have', 'smoke', '20', 'cigarett', 'a', 'day', 'sinc', 'their', 'teen', 'constitut', 'an', 'at-risk', 'group', '.', 'one', 'thing', 'they', '’', 're', 'clearli', 'at', 'risk', 'for', 'is', 'the', 'acut', 'sen', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incit', ',', 'which', 'immedi', 'make', 'a', 'consult', 'ten', '.']\n",
      " \n",
      "Remove stopword & Punctuation\n",
      "['patient', 'late', 'middl', 'age', 'smoke', '20', 'cigarett', 'day', 'sinc', 'teen', 'constitut', 'at-risk', 'group', 'one', 'thing', '’', 'clearli', 'risk', 'acut', 'sen', 'guilt', 'clinician', 'incit', 'immedi', 'make', 'consult', 'ten']\n"
     ]
    }
   ],
   "source": [
    "nltk_process(Example_Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97630161a1b63923bdd9a0aa586c344c3f95acdbb7ddd6c880d3dbef6642c2e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('DL': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
