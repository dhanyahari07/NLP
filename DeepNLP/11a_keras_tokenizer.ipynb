{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOwAdlwU4E0B"
      },
      "outputs": [],
      "source": [
        "#https://machinelearningknowledge.ai/keras-tokenizer-tutorial-with-examples-for-fit_on_texts-texts_to_sequences-texts_to_matrix-sequences_to_matrix/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fit_on_texts method is a part of Keras tokenizer class which is used to update the internal vocabulary for the texts list. We need to call be before using other methods of texts_to_sequences or texts_to_matrix.\n",
        "\n",
        "The object returned by fit_on_texts can be used to derive more information by using the following attributes-\n",
        "\n",
        "word_counts : It is a dictionary of words along with the counts.\n",
        "\n",
        "word_docs : Again a dictionary of words, this tells us how many documents contain this word\n",
        "\n",
        "word_index : In this dictionary, we have unique integers assigned to each word.\n",
        "\n",
        "document_count : This integer count will tell us the total number of documents used for fitting the tokenizer."
      ],
      "metadata": {
        "id": "P32olqdBqLfG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF2s0be84E0H"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "t  = Tokenizer()\n",
        "# Defining 4 document lists\n",
        "fit_text = ['Machine Learning Knowledge',\n",
        "\t    'Machine Learning',\n",
        "            'Deep Learning',\n",
        "            'Artificial Intelligence']\n",
        "\n",
        "t.fit_on_texts(fit_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnWRbhKS4E0J",
        "outputId": "834056fb-6e44-456a-9f85-a84f6e797dbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document count 4\n"
          ]
        }
      ],
      "source": [
        "#The document_count prints the number of documents present in our corpus.  In our above example, there are 4 documents present.\n",
        "print(\"The document count\",t.document_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1a5XOVx4E0K",
        "outputId": "88d81e88-d4b8-4c32-de02-24b6a039e849",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The count of words OrderedDict([('machine', 2), ('learning', 3), ('knowledge', 1), ('deep', 1), ('artificial', 1), ('intelligence', 1)])\n"
          ]
        }
      ],
      "source": [
        "# The word_count shows the number of times words occur in the text corpus passed to the Keras tokenizer class model. In our example, the word ‘machine’ has occurred 2 times, ‘learning’ 3 times, and so on.\n",
        "print(\"The count of words\",t.word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzV47d_Q4E0L",
        "outputId": "dcd3a38b-687f-47df-c95d-7421e14663fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word index {'learning': 1, 'machine': 2, 'knowledge': 3, 'deep': 4, 'artificial': 5, 'intelligence': 6}\n"
          ]
        }
      ],
      "source": [
        "#The word_index assigns a unique index to each word present in the text. This unique integer helps the model during training purposes.\n",
        "print(\"The word index\",t.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zi2_mrZ4E0L",
        "outputId": "741519ac-4fce-45d5-ee71-723764be90c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word docs defaultdict(<class 'int'>, {'knowledge': 1, 'machine': 2, 'learning': 3, 'deep': 1, 'artificial': 1, 'intelligence': 1})\n"
          ]
        }
      ],
      "source": [
        "#The word_doc tells in how many documents each of the words appear. In our example, ‘machine’  appears in 2 documents, ‘learning’ in 1 document, ‘knowledge’ in 3 documents, so on.\n",
        "print(\"The word docs\",t.word_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccAD03mR4E0M"
      },
      "source": [
        "# Example 2 : fit_on_texts on String"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_wjSnzI4E0O"
      },
      "source": [
        "The word_count shows the number of times a character has occurred.\n",
        "The document_count prints the number of characters present in our input text.\n",
        "The word_index assigns a unique index to each character present in the text.\n",
        "The word_docs produces results similar to word_counts and gives the frequency of characters."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fit_on_texts, when applied on a string text, its attributes produce different types of results.\n",
        "\n",
        "The word_count shows the number of times a character has occurred.\n",
        "The document_count prints the number of characters present in our input text.\n",
        "\n",
        "The word_index assigns a unique index to each character present in the text.\n",
        "\n",
        "The word_docs produces results similar to word_counts and gives the frequency of characters."
      ],
      "metadata": {
        "id": "jHc32iJUqZ-0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1WCIgO94E0Q",
        "outputId": "a8f3cedc-c49a-44aa-a7da-a2cc4bbe32f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of characters: OrderedDict([('m', 1), ('a', 2), ('c', 1), ('h', 1), ('i', 2), ('n', 3), ('e', 2), ('l', 1), ('r', 1), ('g', 1)])\n",
            "Length of text: 16\n",
            "Character index {'n': 1, 'a': 2, 'i': 3, 'e': 4, 'm': 5, 'c': 6, 'h': 7, 'l': 8, 'r': 9, 'g': 10}\n",
            "Frequency of characters: defaultdict(<class 'int'>, {'m': 1, 'a': 2, 'c': 1, 'h': 1, 'i': 2, 'n': 3, 'e': 2, 'l': 1, 'r': 1, 'g': 1})\n"
          ]
        }
      ],
      "source": [
        "t  = Tokenizer()\n",
        "\n",
        "fit_text = 'Machine Learning'\n",
        "\n",
        "t.fit_on_texts(fit_text)\n",
        "\n",
        "print(\"Count of characters:\",t.word_counts)\n",
        "print(\"Length of text:\",t.document_count)\n",
        "print(\"Character index\",t.word_index)\n",
        "print(\"Frequency of characters:\",t.word_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X4MMEVb4E0R"
      },
      "source": [
        "# 2. texts_to_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MfjzRYX4E0R"
      },
      "source": [
        "texts_to_sequences method helps in converting tokens of text corpus into a sequence of integers.\n",
        "# Example 1: texts_to_sequences on Document List\n",
        "We can see here in the example that given a corpus of documents, texts_to_sequences assign integers to words. For example, ‘machine’ is assigned value 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijeX0KJH4E0S",
        "outputId": "ab570c1b-3134-4c48-966b-10281d28ee1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sequences generated from text are :  [[2, 1, 3], [2, 1], [4, 1], [5, 6]]\n"
          ]
        }
      ],
      "source": [
        "t = Tokenizer()\n",
        "\n",
        "test_text = ['Machine Learning Knowledge',\n",
        "\t      'Machine Learning',\n",
        "             'Deep Learning',\n",
        "             'Artificial Intelligence']\n",
        "\n",
        "t.fit_on_texts(test_text)\n",
        "\n",
        "sequences = t.texts_to_sequences(test_text)\n",
        "\n",
        "print(\"The sequences generated from text are : \",sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PTD-Z3J4E0T"
      },
      "source": [
        "# Example 2: texts_to_sequences on String\n",
        "In this example, texts_to_sequences assign integers to characters. For example, ‘e’ is assigned a value ‘4’."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9av_hic4E0T",
        "outputId": "8813157b-7070-4962-9a6f-36119bf41abf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sequences generated from text are :  [[5], [2], [6], [7], [3], [1], [4], [], [8], [4], [2], [9], [1], [3], [1], [10]]\n"
          ]
        }
      ],
      "source": [
        "t = Tokenizer()\n",
        "\n",
        "test_text = \"Machine Learning\"\n",
        "\n",
        "t.fit_on_texts(test_text)\n",
        "\n",
        "sequences = t.texts_to_sequences(test_text)\n",
        "\n",
        "print(\"The sequences generated from text are : \",sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0owMZF44E0U"
      },
      "source": [
        "# 3. texts_to_matrix\n",
        "Another useful method of tokenizer class is texts_to_matrix() function for converting the document into a numpy matrix form.\n",
        "\n",
        "This function works in 4 different modes –\n",
        "\n",
        "binary : The default value that tells us about the presence of each word in a document.\n",
        "\n",
        "count : As the name suggests, the count for each word in the document is known.\n",
        "\n",
        "tfidf : The TF-IDF score for each word in the document.\n",
        "\n",
        "freq : The frequency tells us about ratio of words in each document.\n",
        "\n",
        "# Example 1: texts_to_matrix with mode = binary\n",
        "The binary mode in texts_to_matrix() function determines the presence of text by using ‘1’ in the matrix where the word is present and ‘0’ where the word is not present.\n",
        "\n",
        "NOTE: This mode doesn’t count the total number of times a particular word or text, but it just tells about the presence of word in each of the documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJOLNSfg4E0V",
        "outputId": "5e20ecfd-8069-4662-f210-69b582b307bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# define 5 documents\n",
        "docs = ['Marvellous Machine Learning Marvellous Machine Learning',\n",
        "\t\t'Amazing Artificial Intelligence',\n",
        "\t\t'Dazzling Deep Learning',\n",
        "\t\t'Champion Computer Vision',\n",
        "\t\t'Notorious Natural Language Processing Notorious Natural Language Processing']\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "\n",
        "t.fit_on_texts(docs)\n",
        "bias variance tradeoffbias variance tradeoff\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='binary')\n",
        "print(encoded_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G35NwfhO4E0W"
      },
      "source": [
        "# Example 2: texts_to_matrix with mode = count\n",
        "The count mode in texts_to_matrix() function determines the number of times the words appear in each of the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy9OW5w-4E0W",
        "outputId": "7428e5bf-ec68-4b73-8ca4-cd6eceff05e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# define 5 documents \n",
        "docs = ['Marvellous Machine Learning Marvellous Machine Learning', \n",
        "'Amazing Artificial Intelligence', \n",
        "'Dazzling Deep Learning', \n",
        "'Champion Computer Vision', \n",
        "'Notorious Natural Language Processing Notorious Natural Language Processing'] \n",
        "\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "\n",
        "t.fit_on_texts(docs)\n",
        "\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
        "\n",
        "print(encoded_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-W8Bo5i4E0X"
      },
      "source": [
        "# Example 3: texts_to_matrix with mode = tfidf\n",
        "TF-IDF or Term Frequency – Inverse Document Frequency, works by checking the relevance of a word in a given text corpus.\n",
        "\n",
        "In this mode, a proportional score is given to words on the basis of the number of times they occur in the text corpus. In this way, this model can determine which words are worthy and which aren’t.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LLFW5ey4E0X",
        "outputId": "e6748c3c-3f84-499f-a945-6de318dc3b92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         1.8601123  2.48272447 2.48272447 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.46633707 1.46633707 1.46633707 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         1.09861229 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.46633707\n",
            "  1.46633707 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.46633707 1.46633707 1.46633707]\n",
            " [0.         0.         0.         0.         2.48272447 2.48272447\n",
            "  2.48272447 2.48272447 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "t.fit_on_texts(docs)\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='tfidf')\n",
        "print(encoded_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35dxZWMw4E0X"
      },
      "source": [
        "# Example 4: texts_to_matrix with mode = freq\n",
        "This last mode used in texts_to_matrix() is the frequency that actually determines a score and assigning to each on the basis of the ratio of the word with all the words in the document or text corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpCBAAUS4E0Y",
        "outputId": "d3f255c8-c948-4b41-bfd3-3d74743a679c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.33333333 0.33333333 0.33333333 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.33333333 0.33333333 0.33333333 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.33333333 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.33333333\n",
            "  0.33333333 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.33333333 0.33333333 0.33333333]\n",
            " [0.         0.         0.         0.         0.25       0.25\n",
            "  0.25       0.25       0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "t.fit_on_texts(docs)\n",
        "\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='freq')\n",
        "\n",
        "print(encoded_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLlfr6IU4E0Y"
      },
      "source": [
        "# 4. sequences_to_matrix\n",
        "This function sequences_to_matrix() of Keras tokenizer class is used to convert the sequences into a numpy matrix form.\n",
        "\n",
        "sequences_to_matrix also has 4 different modes to work with –\n",
        "\n",
        "binary : The default value that tells us about the presence of each word in a document.\n",
        "count : As the name suggests, the count for each word in the document is known.\n",
        "tfidf : The TF-IDF score for each word in the document.\n",
        "freq : The frequency tells us about ratio of words in each document.\n",
        "# Example 1: sequences_to_matrix with mode = binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg5f29no4E0Z",
        "outputId": "0ece7c60-667c-4470-fdc3-07d33c12f0f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 1. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 1. 1. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 1.]]\n"
          ]
        }
      ],
      "source": [
        "# define 4 documents\n",
        "docs =['Machine Learning Knowledge',\n",
        "       'Machine Learning and Deep Learning',\n",
        "       'Deep Learning',\n",
        "       'Artificial Intelligence']\n",
        "\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "\n",
        "t.fit_on_texts(docs)\n",
        "\n",
        "sequences = t.texts_to_sequences(docs)\n",
        "\n",
        "encoded_docs = t.sequences_to_matrix(sequences, mode='binary')\n",
        "print(encoded_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JCHte7P4E0Z"
      },
      "source": [
        "# Example 2: sequences_to_matrix with mode = count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jPhQp1g4E0Z",
        "outputId": "ed8ca252-4e1d-49e4-cd59-6e728077931b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 1. 0. 1. 0. 0. 0.]\n",
            " [0. 2. 1. 1. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 1.]]\n"
          ]
        }
      ],
      "source": [
        "# define 4 documents\n",
        "docs =['Machine Learning Knowledge',\n",
        "       'Machine Learning and Deep Learning',\n",
        "      'Deep Learning',\n",
        "      'Artificial Intelligence']\n",
        "\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "\n",
        "t.fit_on_texts(docs)\n",
        "\n",
        "sequences = t.texts_to_sequences(docs)\n",
        "\n",
        "encoded_docs = t.sequences_to_matrix(sequences, mode='count')\n",
        "print(encoded_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D95C__Kg4E0Z"
      },
      "source": [
        "# Example 3: sequences_to_matrix with mode = tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6saabc64E0a",
        "outputId": "3cb6da37-3b0e-4f27-cc78-e75b389d3445",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.69314718 0.84729786 0.         1.09861229 0.\n",
            "  0.         0.        ]\n",
            " [0.         1.17360019 0.84729786 0.84729786 0.         1.09861229\n",
            "  0.         0.        ]\n",
            " [0.         0.69314718 0.         0.84729786 0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  1.09861229 1.09861229]]\n"
          ]
        }
      ],
      "source": [
        "# define 4 documents\n",
        "docs =['Machine Learning Knowledge',\n",
        "      'Machine Learning and Deep Learning',\n",
        "      'Deep Learning',\n",
        "      'Artificial Intelligence']\n",
        "\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "\n",
        "t.fit_on_texts(docs)\n",
        "\n",
        "sequences = t.texts_to_sequences(docs)\n",
        "\n",
        "encoded_docs = t.sequences_to_matrix(sequences, mode='tfidf')\n",
        "print(encoded_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pjd4Ckjf4E0a"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AugB2jZ4E0a"
      },
      "source": [
        "# Example 4: sequences_to_matrix with mode = freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDIVSECX4E0a",
        "outputId": "6ec6b1ad-0eeb-4fe7-f8a0-e94581d565c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.33333333 0.33333333 0.         0.33333333 0.\n",
            "  0.         0.        ]\n",
            " [0.         0.4        0.2        0.2        0.         0.2\n",
            "  0.         0.        ]\n",
            " [0.         0.5        0.         0.5        0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.5        0.5       ]]\n"
          ]
        }
      ],
      "source": [
        "# define 4 documents\n",
        "docs =['Machine Learning Knowledge',\n",
        "       'Machine Learning and Deep Learning',\n",
        "       'Deep Learning',\n",
        "       'Artificial Intelligence']\n",
        "\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "\n",
        "t.fit_on_texts(docs)\n",
        "\n",
        "sequences = t.texts_to_sequences(docs)\n",
        "\n",
        "encoded_docs = t.sequences_to_matrix(sequences, mode='freq')\n",
        "print(encoded_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL4bTl4m4E0b"
      },
      "outputs": [],
      "source": [
        "#https://machinelearningknowledge.ai/keras-tokenizer-tutorial-with-examples-for-fit_on_texts-texts_to_sequences-texts_to_matrix-sequences_to_matrix/"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "97630161a1b63923bdd9a0aa586c344c3f95acdbb7ddd6c880d3dbef6642c2e8"
    },
    "kernelspec": {
      "display_name": "Python 3.9.9 64-bit ('DL': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "11a. keras_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}