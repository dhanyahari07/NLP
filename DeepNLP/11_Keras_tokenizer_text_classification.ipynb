{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11. Keras_tokenizer_text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4cUhKGYPhd0kb5wR3Nwim"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.datasets import reuters\n",
        "import tensorflow as tf\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "#word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "\n",
        "print('# of Training Samples: {}'.format(len(x_train)))\n",
        "print('# of Test Samples: {}'.format(len(x_test)))\n",
        "\n",
        "num_classes = max(y_train) + 1\n",
        "print('# of Classes: {}'.format(num_classes))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_words = 10000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(x_train[0])\n",
        "print(len(x_train[0]))\n",
        "print(max(x_train[0]))\n",
        "\n",
        "print(y_train[0])\n",
        "print(len(y_train[0]))\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.metrics_names)\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='count')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='count')\n",
        "\n",
        "y_train =tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(x_train[0])\n",
        "print(len(x_train[0]))\n",
        "print(max(x_train[0]))\n",
        "print(np.argmax(x_train[0]))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='freq')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='freq')\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(x_train[0])\n",
        "print(len(x_train[0]))\n",
        "print(max(x_train[0]))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data\n",
        "(num_words=None, test_split=0.2)\n",
        "\n",
        "tokenizer.fit_on_sequences(x_train)\n",
        "\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='tfidf')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='tfidf')\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(x_train[0])\n",
        "print(len(x_train[0]))\n",
        "print(max(x_train[0]))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbN99aaA4pZD",
        "outputId": "a08ed801-cdad-439e-9ccf-0a0c5697ab37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Training Samples: 8982\n",
            "# of Test Samples: 2246\n",
            "# of Classes: 46\n",
            "[0. 1. 0. ... 0. 0. 0.]\n",
            "10000\n",
            "1.0\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "46\n",
            "[]\n",
            "Epoch 1/2\n",
            "253/253 [==============================] - 13s 48ms/step - loss: 1.2676 - accuracy: 0.7261 - val_loss: 0.9640 - val_accuracy: 0.7931\n",
            "Epoch 2/2\n",
            "253/253 [==============================] - 11s 42ms/step - loss: 0.4887 - accuracy: 0.8888 - val_loss: 0.8666 - val_accuracy: 0.8209\n",
            "71/71 [==============================] - 1s 12ms/step - loss: 0.8223 - accuracy: 0.8050\n",
            "Test loss: 0.822298526763916\n",
            "Test accuracy: 0.8049866557121277\n",
            "[0. 1. 0. ... 0. 0. 0.]\n",
            "10000\n",
            "6.0\n",
            "6\n",
            "Epoch 1/2\n",
            "253/253 [==============================] - 12s 46ms/step - loss: 1.3088 - accuracy: 0.7313 - val_loss: 0.9946 - val_accuracy: 0.7920\n",
            "Epoch 2/2\n",
            "253/253 [==============================] - 13s 51ms/step - loss: 0.5525 - accuracy: 0.8802 - val_loss: 0.9232 - val_accuracy: 0.8142\n",
            "71/71 [==============================] - 1s 11ms/step - loss: 0.9220 - accuracy: 0.8054\n",
            "Test loss: 0.922016978263855\n",
            "Test accuracy: 0.8054319024085999\n",
            "[0.         0.01149425 0.         ... 0.         0.         0.        ]\n",
            "10000\n",
            "0.06896551724137931\n",
            "Epoch 1/2\n",
            "253/253 [==============================] - 19s 74ms/step - loss: 2.2898 - accuracy: 0.4668 - val_loss: 1.8749 - val_accuracy: 0.4983\n",
            "Epoch 2/2\n",
            "253/253 [==============================] - 14s 56ms/step - loss: 1.6832 - accuracy: 0.5684 - val_loss: 1.6011 - val_accuracy: 0.6518\n",
            "71/71 [==============================] - 1s 16ms/step - loss: 1.6033 - accuracy: 0.6407\n",
            "Test loss: 1.6033053398132324\n",
            "Test accuracy: 0.6406945586204529\n",
            "[0.         0.69309152 0.         ... 0.         0.         0.        ]\n",
            "10000\n",
            "6.214608098422191\n",
            "Epoch 1/2\n",
            "253/253 [==============================] - 13s 51ms/step - loss: 1.2665 - accuracy: 0.7495 - val_loss: 0.9522 - val_accuracy: 0.8198\n",
            "Epoch 2/2\n",
            "253/253 [==============================] - 15s 61ms/step - loss: 0.4433 - accuracy: 0.9134 - val_loss: 1.0574 - val_accuracy: 0.8020\n",
            "71/71 [==============================] - 1s 19ms/step - loss: 1.0417 - accuracy: 0.8041\n",
            "Test loss: 1.0416918992996216\n",
            "Test accuracy: 0.8040961623191833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.datasets import reuters\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "r4sXNKt0OhgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) \n",
        "= reuters.load_data(num_words=None, test_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsro3ZPcOl5B",
        "outputId": "8885005a-ffde-464d-ad7a-48d42843ab66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n",
            "2121728/2110848 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('# of Training Samples: {}'.format(len(x_train)))\n",
        "print('# of Test Samples: {}'.format(len(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f77gTYHSOl7k",
        "outputId": "9dc24984-9d0a-469f-da77-3ccd9e2eff71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Training Samples: 8982\n",
            "# of Test Samples: 2246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = max(y_train) + 1\n",
        "print('# of Classes: {}'.format(num_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYgVR1hnOl-I",
        "outputId": "830b81ab-0ca6-4a2b-f40b-983f4fe7c9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Classes: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_words = 10000"
      ],
      "metadata": {
        "id": "cwjN6mTmOmAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')"
      ],
      "metadata": {
        "id": "AE2CxlkgOmCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "o_w-KL2LO2hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n",
        "print(len(x_train[0]))\n",
        "print(max(x_train[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib-ZwvngO6c2",
        "outputId": "e4aa9806-04f3-4047-ef1f-1f827f65dcf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 0. ... 0. 0. 0.]\n",
            "10000\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[0])\n",
        "print(len(y_train[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8MO1UEeO-Zq",
        "outputId": "e65f6b5f-e14b-484e-ef4e-8f91cad3d1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation"
      ],
      "metadata": {
        "id": "6u1vVmaWPEQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "4EFzbuxfPII5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "#print(model.metrics)"
      ],
      "metadata": {
        "id": "3eYBe8gTPMNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 2"
      ],
      "metadata": {
        "id": "RLtYTFaIPQrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, batch_size=batch_size,\n",
        "                    epochs=epochs, verbose=1, validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test, batch_size=batch_size,\n",
        "                       verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dImJ6l-dPU5C",
        "outputId": "d1b098bc-84a8-4cfb-9c26-ae70cacdd8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "253/253 [==============================] - 11s 41ms/step - loss: 1.3028 - accuracy: 0.7213 - val_loss: 0.9543 - val_accuracy: 0.8020\n",
            "Epoch 2/2\n",
            "253/253 [==============================] - 9s 36ms/step - loss: 0.4939 - accuracy: 0.8884 - val_loss: 0.8874 - val_accuracy: 0.8053\n",
            "71/71 [==============================] - 1s 10ms/step - loss: 0.8527 - accuracy: 0.8010\n",
            "Test loss: 0.8526968955993652\n",
            "Test accuracy: 0.800979495048523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)"
      ],
      "metadata": {
        "id": "GCoGkhE-Pfrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='tfidf')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='tfidf')"
      ],
      "metadata": {
        "id": "X7ORLRkOTym4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_train =tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "k1ElE1cwTypN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n",
        "print(len(x_train[0]))\n",
        "print(max(x_train[0]))\n",
        "print(np.argmax(x_train[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjLJrCtXTyrz",
        "outputId": "9fc29591-85f2-4b07-bb39-db169ea48e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 0. ... 0. 0. 0.]\n",
            "10000\n",
            "6.0\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PEeM9nviTyuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RCcGFAiTyxC",
        "outputId": "287691e6-598a-4681-e3a7-da0e40f58d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "253/253 [==============================] - 9s 32ms/step - loss: 1.3080 - accuracy: 0.7296 - val_loss: 0.9794 - val_accuracy: 0.8042\n",
            "Epoch 2/2\n",
            "253/253 [==============================] - 8s 31ms/step - loss: 0.5564 - accuracy: 0.8786 - val_loss: 0.8741 - val_accuracy: 0.8198\n",
            "71/71 [==============================] - 1s 10ms/step - loss: 0.8873 - accuracy: 0.8014\n",
            "Test loss: 0.8872594237327576\n",
            "Test accuracy: 0.8014247417449951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)"
      ],
      "metadata": {
        "id": "M7TkKf76OmFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='count')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='count')"
      ],
      "metadata": {
        "id": "M7-savCoURoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "fD0b0kyJURqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n",
        "print(len(x_train[0]))\n",
        "print(max(x_train[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0Vt4xxHURs8",
        "outputId": "3dbdd657-2bb0-4cb7-8f5e-b507d0613e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.         0.01149425 0.         ... 0.         0.         0.        ]\n",
            "10000\n",
            "0.06896551724137931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Na_D-BMHURvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLMEgYpeURx3",
        "outputId": "d3c3553a-c6f6-4fdb-aa0e-a419ee8e266f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "253/253 [==============================] - 8s 32ms/step - loss: 2.3023 - accuracy: 0.4439 - val_loss: 1.8896 - val_accuracy: 0.5006\n",
            "Epoch 2/2\n",
            "253/253 [==============================] - 8s 31ms/step - loss: 1.6969 - accuracy: 0.5650 - val_loss: 1.6160 - val_accuracy: 0.6463\n",
            "71/71 [==============================] - 1s 9ms/step - loss: 1.6137 - accuracy: 0.6385\n",
            "Test loss: 1.6137014627456665\n",
            "Test accuracy: 0.6384683847427368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "\n",
        "tokenizer.fit_on_sequences(x_train)\n",
        "\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='tfidf')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='tfidf')\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(x_train[0])\n",
        "print(len(x_train[0]))\n",
        "print(max(x_train[0]))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwGOvnH5UR0X",
        "outputId": "9343e34b-5d2e-402e-f408-260a43e1b111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.         0.69309152 0.         ... 0.         0.         0.        ]\n",
            "10000\n",
            "6.214608098422191\n",
            "Epoch 1/2\n",
            "253/253 [==============================] - 8s 32ms/step - loss: 1.2373 - accuracy: 0.7545 - val_loss: 0.9161 - val_accuracy: 0.8198\n",
            "Epoch 2/2\n",
            "253/253 [==============================] - 8s 31ms/step - loss: 0.4523 - accuracy: 0.9162 - val_loss: 1.0805 - val_accuracy: 0.7987\n",
            "71/71 [==============================] - 1s 9ms/step - loss: 1.0512 - accuracy: 0.8094\n",
            "Test loss: 1.0511690378189087\n",
            "Test accuracy: 0.8094390034675598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LzIE1kbwUmx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xlf759JPUm0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PFU7O5ZYUm3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1HYFtTRtUm59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gsx2PugAUm8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vKklRbNvUR3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MdNT9D7NOmIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://towardsdatascience.com/text-classification-in-keras-part-1-a-simple-reuters-news-classifier-9558d34d01d3\n",
        "# https://towardsdatascience.com/text-classification-in-keras-part-2-how-to-use-the-keras-tokenizer-word-representations-fd571674df23"
      ],
      "metadata": {
        "id": "fVxbKDvR4-Tf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}