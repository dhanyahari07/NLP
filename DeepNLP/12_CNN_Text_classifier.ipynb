{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS8BD7bn5hww",
        "outputId": "c3b9c815-209e-433b-de72-d6b345bacc1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "1\n",
            "Shape of training data: \n",
            "(25000,)\n",
            "(25000,)\n",
            "Shape of test data: \n",
            "(25000,)\n",
            "(25000,)\n"
          ]
        }
      ],
      "source": [
        "# Importing libraries\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "# Our dictionary will contain only of the top 7000 words appearing most frequently\n",
        "top_words = 7000\n",
        "# Now we split our data-set into training and test data\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "# Looking at the nature of training data\n",
        "print(X_train[0])\n",
        "print(y_train[0])\n",
        "print('Shape of training data: ')\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print('Shape of test data: ')\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6HTQDG05hw2"
      },
      "outputs": [],
      "source": [
        "# Padding the data samples to a maximum review length in words\n",
        "max_words = 450\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
        "# Building the CNN Model\n",
        "model = Sequential()      # initilaizing the Sequential nature for CNN model\n",
        "# Adding the embedding layer which will take in maximum of 450 words as input and provide a 32 dimensional output of those words which belong in the top_words dictionary\n",
        "model.add(Embedding(top_words, 32, input_length=max_words))\n",
        "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh64nc-t5hw3",
        "outputId": "d72b1f46-bb34-438b-ba5f-16699accbbd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 450, 32)           224000    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 450, 32)           3104      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 225, 32)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 7200)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 250)               1800250   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 251       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,027,605\n",
            "Trainable params: 2,027,605\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD2Xi9uh5hw4",
        "outputId": "7e8822f4-84cb-4141-d2a7-325b64e833f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "196/196 - 26s - loss: 0.4529 - accuracy: 0.7586 - val_loss: 0.2773 - val_accuracy: 0.8836 - 26s/epoch - 134ms/step\n",
            "Epoch 2/2\n",
            "196/196 - 25s - loss: 0.1968 - accuracy: 0.9242 - val_loss: 0.2771 - val_accuracy: 0.8847 - 25s/epoch - 127ms/step\n",
            "Accuracy: 88.47%\n"
          ]
        }
      ],
      "source": [
        "# Fitting the data onto model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
        "# Getting score metrics from our model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "# Displays the accuracy of correct sentiment prediction over test data\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmUBwct35hw5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "97630161a1b63923bdd9a0aa586c344c3f95acdbb7ddd6c880d3dbef6642c2e8"
    },
    "kernelspec": {
      "display_name": "Python 3.9.9 64-bit ('DL': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "12. CNN_Text_classifier.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}